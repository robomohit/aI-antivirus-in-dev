#!/usr/bin/env python3
"""
Train Model with Real MalwareBazaar Data
Train the modernized model using real malware samples
"""

import os
import sys
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from colorama import init, Fore, Style
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# Initialize colorama
init()

class RealMalwareTrainer:
    def __init__(self):
        self.features_dir = "malware_features"
        self.models_dir = "real_malware_models"
        
        # Create models directory
        Path(self.models_dir).mkdir(exist_ok=True)
        
        self.setup_logging()
    
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
    
    def find_latest_dataset(self):
        """Find the latest dataset file."""
        dataset_dir = Path("malware_features")
        if not dataset_dir.exists():
            raise FileNotFoundError("malware_features directory not found")
        
        # Look for realistic dataset first, then any other dataset
        csv_files = list(dataset_dir.glob("*.csv"))
        if not csv_files:
            raise FileNotFoundError("No CSV datasets found in malware_features directory")
        
        # Sort by modification time (newest first)
        csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        # Prefer realistic dataset if available
        for file in csv_files:
            if "realistic" in file.name:
                print(f"{Fore.GREEN}âœ… Found realistic dataset: {file.name}")
                return file
        
        # Otherwise use the newest file
        latest_file = csv_files[0]
        print(f"{Fore.GREEN}âœ… Found dataset: {latest_file.name}")
        return latest_file
    
    def load_dataset(self, dataset_path):
        """Load and prepare the dataset."""
        print(f"{Fore.CYAN}ğŸ”„ Loading dataset: {dataset_path}")
        
        try:
            df = pd.read_csv(dataset_path)
            print(f"{Fore.GREEN}âœ… Dataset loaded successfully!")
            print(f"{Fore.CYAN}ğŸ“Š Dataset shape: {df.shape}")
            print(f"{Fore.CYAN}ğŸ“Š Columns: {list(df.columns)}")
            
            # Check for required columns
            required_cols = ['file_size', 'entropy', 'strings_count', 'avg_string_length', 
                           'printable_ratio', 'histogram_regularity', 'entropy_consistency', 'label']
            
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"{Fore.RED}âŒ Missing required columns: {missing_cols}")
                return None
            
            # Show class distribution
            malware_count = len(df[df['label'] == 1])
            benign_count = len(df[df['label'] == 0])
            print(f"{Fore.CYAN}ğŸ“Š Malware samples: {malware_count}")
            print(f"{Fore.CYAN}ğŸ“Š Benign samples: {benign_count}")
            
            return df
            
        except Exception as e:
            print(f"{Fore.RED}âŒ Error loading dataset: {e}")
            return None
    
    def prepare_features(self, df):
        """Prepare features for training."""
        print(f"{Fore.CYAN}ğŸ”„ Preparing features...")
        
        # Select feature columns (exclude metadata)
        feature_cols = [
            'file_size', 'entropy', 'strings_count', 'avg_string_length',
            'printable_ratio', 'histogram_regularity', 'entropy_consistency'
        ]
        
        # Check if all features exist
        missing_features = [col for col in feature_cols if col not in df.columns]
        if missing_features:
            print(f"{Fore.RED}âŒ Missing features: {missing_features}")
            return None, None, None
        
        # Prepare X and y
        X = df[feature_cols].copy()
        y = df['label'].copy()
        
        # Handle missing values
        X = X.fillna(0)
        
        # Remove infinite values
        X = X.replace([np.inf, -np.inf], 0)
        
        print(f"{Fore.GREEN}âœ… Features prepared!")
        print(f"{Fore.CYAN}ğŸ“Š Feature matrix shape: {X.shape}")
        print(f"{Fore.CYAN}ğŸ“Š Target vector shape: {y.shape}")
        
        return X, y, feature_cols
    
    def train_model(self, X, y, feature_cols):
        """Train the model with real malware data."""
        print(f"\n{Fore.CYAN}ğŸ”„ Training model with real malware data...")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"{Fore.CYAN}ğŸ“Š Training samples: {len(X_train)}")
        print(f"{Fore.CYAN}ğŸ“Š Test samples: {len(X_test)}")
        print(f"{Fore.CYAN}ğŸ“Š Features: {len(feature_cols)}")
        
        # LightGBM parameters (optimized for real malware)
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42,
            # Anti-overfitting parameters
            'min_data_in_leaf': 20,
            'min_gain_to_split': 0.1,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'max_depth': 6,
            'early_stopping_rounds': 50
        }
        
        # Create dataset
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
        
        # Train model
        model = lgb.train(
            params,
            train_data,
            valid_sets=[valid_data],
            num_boost_round=1000,
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
        )
        
        return model, X_test, y_test, feature_cols
    
    def evaluate_model(self, model, X_test, y_test):
        """Evaluate the trained model."""
        print(f"\n{Fore.CYAN}ğŸ“Š MODEL EVALUATION:")
        print(f"{Fore.CYAN}{'='*40}")
        
        # Make predictions
        y_pred_proba = model.predict(X_test)
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # Classification report
        report = classification_report(y_test, y_pred, target_names=['Benign', 'Malware'])
        print(f"{Fore.GREEN}Classification Report:")
        print(report)
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        print(f"{Fore.GREEN}Confusion Matrix:")
        print(f"True Negatives: {cm[0,0]} (Benign files correctly identified)")
        print(f"False Positives: {cm[0,1]} (Benign files incorrectly flagged)")
        print(f"False Negatives: {cm[1,0]} (Malware files missed)")
        print(f"True Positives: {cm[1,1]} (Malware files correctly detected)")
        
        # Calculate metrics
        accuracy = (cm[0,0] + cm[1,1]) / cm.sum()
        precision = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
        recall = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        auc_score = roc_auc_score(y_test, y_pred_proba)
        
        print(f"\n{Fore.GREEN}ğŸ“Š Performance Metrics:")
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Precision: {precision:.3f}")
        print(f"   Recall: {recall:.3f}")
        print(f"   F1-Score: {f1_score:.3f}")
        print(f"   AUC Score: {auc_score:.3f}")
        
        # Calculate false positive rate
        fpr = cm[0,1] / (cm[0,0] + cm[0,1]) if (cm[0,0] + cm[0,1]) > 0 else 0
        print(f"   False Positive Rate: {fpr:.3f}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'auc_score': auc_score,
            'false_positive_rate': fpr
        }
    
    def cross_validate_model(self, X, y):
        """Perform cross-validation."""
        print(f"\n{Fore.CYAN}ğŸ”„ Performing cross-validation...")
        
        # LightGBM parameters
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42,
            'min_data_in_leaf': 20,
            'min_gain_to_split': 0.1,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'max_depth': 6
        }
        
        # Perform cross-validation
        cv_scores = cross_val_score(
            lgb.LGBMClassifier(**params),
            X, y,
            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
            scoring='accuracy'
        )
        
        print(f"{Fore.GREEN}ğŸ“Š Cross-Validation Results:")
        print(f"   Mean Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
        print(f"   Individual Scores: {[f'{score:.3f}' for score in cv_scores]}")
        
        return cv_scores
    
    def save_model(self, model, feature_cols, metrics, dataset_info):
        """Save the trained model."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save model
        model_path = Path(self.models_dir) / f"real_malware_model_{timestamp}.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        # Save metadata
        metadata_path = Path(self.models_dir) / f"real_malware_metadata_{timestamp}.pkl"
        metadata = {
            'feature_cols': feature_cols,
            'training_date': timestamp,
            'model_type': 'LightGBM',
            'version': 'real_malware_2025',
            'description': 'Model trained on real MalwareBazaar data',
            'metrics': metrics,
            'dataset_info': dataset_info
        }
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"\n{Fore.GREEN}âœ… Model saved!")
        print(f"{Fore.CYAN}ğŸ“ Model: {model_path}")
        print(f"{Fore.CYAN}ğŸ“ Metadata: {metadata_path}")
        
        return model_path, metadata_path
    
    def run_training_process(self):
        """Run the complete training process."""
        print(f"{Fore.CYAN}ğŸ›¡ï¸  REAL MALWARE TRAINING PROCESS")
        print(f"{Fore.CYAN}{'='*50}")
        
        # Step 1: Find and load dataset
        dataset_path = self.find_latest_dataset()
        if not dataset_path:
            return None
        
        df = self.load_dataset(dataset_path)
        if df is None:
            return None
        
        # Step 2: Prepare features
        X, y, feature_cols = self.prepare_features(df)
        if X is None:
            return None
        
        # Step 3: Cross-validation
        cv_scores = self.cross_validate_model(X, y)
        
        # Step 4: Train model
        model, X_test, y_test, feature_cols = self.train_model(X, y, feature_cols)
        
        # Step 5: Evaluate model
        metrics = self.evaluate_model(model, X_test, y_test)
        
        # Step 6: Save model
        dataset_info = {
            'total_samples': len(df),
            'malware_samples': len(df[df['label'] == 1]),
            'benign_samples': len(df[df['label'] == 0]),
            'features_count': len(feature_cols),
            'cv_mean_accuracy': cv_scores.mean(),
            'cv_std_accuracy': cv_scores.std()
        }
        
        model_path, metadata_path = self.save_model(model, feature_cols, metrics, dataset_info)
        
        print(f"\n{Fore.GREEN}ğŸ‰ TRAINING COMPLETE!")
        print(f"{Fore.CYAN}ğŸ“Š Model trained on real malware data")
        print(f"{Fore.CYAN}ğŸ“Š Cross-validation accuracy: {cv_scores.mean():.3f}")
        print(f"{Fore.CYAN}ğŸ“Š Test accuracy: {metrics['accuracy']:.3f}")
        print(f"{Fore.CYAN}ğŸ“Š False positive rate: {metrics['false_positive_rate']:.3f}")
        
        return model_path, metadata_path

def main():
    """Main function."""
    print(f"{Fore.CYAN}ğŸ›¡ï¸  Starting Real Malware Training...")
    
    trainer = RealMalwareTrainer()
    trainer.run_training_process()

if __name__ == "__main__":
    main()